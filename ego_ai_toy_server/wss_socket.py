import asyncio
import time
import uuid

import aiohttp
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
import uvicorn
import json
import base64
import wave
import os
import torch
import torchaudio
import numpy as np

from funasr import AutoModel
from pydub import AudioSegment

import config
# from mysql_connect import MySQLDataInserter
from doubao import DouBao_chat
from tengxun_tts import synthesize_audio_stream, process
# generate_audio
from xinghuo_tts_utils import generate_audio
model = "iic/SenseVoiceSmall"
model = AutoModel(model=model,
                  vad_model="iic/speech_fsmn_vad_zh-cn-16k-common-pytorch",
                  vad_kwargs={"max_single_segment_time": 30000},
                  trust_remote_code=True,
                  )
doubao = DouBao_chat()

class TOY():
    def __init__(self):
        self.is_playing = False
        self.client = ""
        self.user_sheding = []
        self.prompt_text = ""
        self.platform = ""
        self.device_id = ""
        self.toy_role = ""


emo_dict = {
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
}

event_dict = {
    "<|BGM|>": "ğŸ¼",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜€",
    "<|Cry|>": "ğŸ˜­",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ¤§",
}

emoji_dict = {
    "<|nospeech|><|Event_UNK|>": "â“",
    "<|zh|>": "",
    "<|en|>": "",
    "<|yue|>": "",
    "<|ja|>": "",
    "<|ko|>": "",
    "<|nospeech|>": "",
    "<|HAPPY|>": "ğŸ˜Š",
    "<|SAD|>": "ğŸ˜”",
    "<|ANGRY|>": "ğŸ˜¡",
    "<|NEUTRAL|>": "",
    "<|BGM|>": "ğŸ¼",
    "<|Speech|>": "",
    "<|Applause|>": "ğŸ‘",
    "<|Laughter|>": "ğŸ˜€",
    "<|FEARFUL|>": "ğŸ˜°",
    "<|DISGUSTED|>": "ğŸ¤¢",
    "<|SURPRISED|>": "ğŸ˜®",
    "<|Cry|>": "ğŸ˜­",
    "<|EMO_UNKNOWN|>": "",
    "<|Sneeze|>": "ğŸ¤§",
    "<|Breath|>": "",
    "<|Cough|>": "ğŸ˜·",
    "<|Sing|>": "",
    "<|Speech_Noise|>": "",
    "<|withitn|>": "",
    "<|woitn|>": "",
    "<|GBG|>": "",
    "<|Event_UNK|>": "",
}

lang_dict =  {
    "<|zh|>": "<|lang|>",
    "<|en|>": "<|lang|>",
    "<|yue|>": "<|lang|>",
    "<|ja|>": "<|lang|>",
    "<|ko|>": "<|lang|>",
    "<|nospeech|>": "<|lang|>",
}

emo_set = {"ğŸ˜Š", "ğŸ˜”", "ğŸ˜¡", "ğŸ˜°", "ğŸ¤¢", "ğŸ˜®"}
event_set = {"ğŸ¼", "ğŸ‘", "ğŸ˜€", "ğŸ˜­", "ğŸ¤§", "ğŸ˜·",}

def format_str(s):
    for sptk in emoji_dict:
        s = s.replace(sptk, emoji_dict[sptk])
    return s


def format_str_v2(s):
    sptk_dict = {}
    for sptk in emoji_dict:
        sptk_dict[sptk] = s.count(sptk)
        s = s.replace(sptk, "")
    emo = "<|NEUTRAL|>"
    for e in emo_dict:
        if sptk_dict[e] > sptk_dict[emo]:
            emo = e
    for e in event_dict:
        if sptk_dict[e] > 0:
            s = event_dict[e] + s
    s = s + emo_dict[emo]

    for emoji in emo_set.union(event_set):
        s = s.replace(" " + emoji, emoji)
        s = s.replace(emoji + " ", emoji)
    return s.strip()

def format_str_v3(s):
    def get_emo(s):
        return s[-1] if s[-1] in emo_set else None
    def get_event(s):
        return s[0] if s[0] in event_set else None

    s = s.replace("<|nospeech|><|Event_UNK|>", "â“")
    for lang in lang_dict:
        s = s.replace(lang, "<|lang|>")
    s_list = [format_str_v2(s_i).strip(" ") for s_i in s.split("<|lang|>")]
    new_s = " " + s_list[0]
    cur_ent_event = get_event(new_s)
    for i in range(1, len(s_list)):
        if len(s_list[i]) == 0:
            continue
        if get_event(s_list[i]) == cur_ent_event and get_event(s_list[i]) != None:
            s_list[i] = s_list[i][1:]
        #else:
        cur_ent_event = get_event(s_list[i])
        if get_emo(s_list[i]) != None and get_emo(s_list[i]) == get_emo(new_s):
            new_s = new_s[:-1]
        new_s += s_list[i].strip().lstrip()
    new_s = new_s.replace("The.", " ")
    return new_s.strip()

def model_inference(input_wav, language, fs=16000):
    # task_abbr = {"Speech Recognition": "ASR", "Rich Text Transcription": ("ASR", "AED", "SER")}
    language_abbr = {"auto": "auto", "zh": "zh", "en": "en", "yue": "yue", "ja": "ja", "ko": "ko",
                     "nospeech": "nospeech"}

    # task = "Speech Recognition" if task is None else task
    language = "auto" if len(language) < 1 else language
    selected_language = language_abbr[language]
    # selected_task = task_abbr.get(task)

    # print(f"input_wav: {type(input_wav)}, {input_wav[1].shape}, {input_wav}")

    if isinstance(input_wav, tuple):
        fs, input_wav = input_wav
        input_wav = input_wav.astype(np.float32) / np.iinfo(np.int16).max
        if len(input_wav.shape) > 1:
            input_wav = input_wav.mean(-1)
        if fs != 16000:
            print(f"audio_fs: {fs}")
            resampler = torchaudio.transforms.Resample(fs, 16000)
            input_wav_t = torch.from_numpy(input_wav).to(torch.float32)
            input_wav = resampler(input_wav_t[None, :])[0, :].numpy()


    merge_vad = True #False if selected_task == "ASR" else True
    print(f"language: {language}, merge_vad: {merge_vad}")
    text = model.generate(input=input_wav,
                          cache={},
                          language=language,
                          use_itn=True,
                          batch_size_s=60, merge_vad=merge_vad)

    print(text)
    text = text[0]["text"]
    text = format_str_v3(text)

    print(text)

    return text


def mp3_to_wav(mp3_path):
    """
    å°† MP3 éŸ³é¢‘æ–‡ä»¶è½¬æ¢ä¸ºåŒåçš„ WAV æ ¼å¼ï¼Œå¹¶åˆ é™¤æº MP3 æ–‡ä»¶ã€‚

    :param mp3_path: MP3 æ–‡ä»¶çš„è·¯å¾„
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸º MP3 æ ¼å¼
        if not mp3_path.lower().endswith('.mp3'):
            print(f"{mp3_path} ä¸æ˜¯ MP3 æ–‡ä»¶ï¼Œè·³è¿‡è½¬æ¢ã€‚")
            return mp3_path

        # ç”Ÿæˆå¯¹åº”çš„ WAV æ–‡ä»¶è·¯å¾„
        wav_path = os.path.splitext(mp3_path)[0] + '.wav'

        # è¯»å– MP3 æ–‡ä»¶
        audio = AudioSegment.from_mp3(mp3_path)

        # ä¿å­˜ä¸º WAV æ ¼å¼
        audio.export(wav_path, format="wav")
        print(f"æˆåŠŸå°† {mp3_path} è½¬æ¢ä¸º {wav_path}")

        # åˆ é™¤æº MP3 æ–‡ä»¶
        os.remove(mp3_path)
        print(f"å·²åˆ é™¤æºæ–‡ä»¶ {mp3_path}")
        return wav_path
    except Exception as e:
        print(f"è½¬æ¢è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")

async def send_audio_over_ws_tengxun(params, audio_file_path):
    if params["platform"] == "tx":
        result = process(params["text"], audio_file_path, params["voice_type"])+".wav"
    elif params["platform"] == "xh":
        result = generate_audio(params["text"], params["voice_type"], audio_file_path+".mp3")
        # with ThreadPoolExecutor() as executor:
        #     # ä½¿ç”¨ zip å‡½æ•°å°†ä¸‰ä¸ªåˆ—è¡¨æ‰“åŒ…ï¼Œç„¶åä½¿ç”¨ map æ–¹æ³•å¹¶å‘è°ƒç”¨ generate_audio æ–¹æ³•
        #     results = list(executor.map(generate_audio, texts, res_ids, audio_paths))
    out_pout_path = mp3_to_wav(result)
    print(f"\nTask {result} completed\n")
    # for inner_generator in outer_generator:
    #     # éå†å†…å±‚ç”Ÿæˆå™¨ï¼Œè·å–éŸ³é¢‘æ•°æ®å—
    #     for audio_chunk in inner_generator:
    #         print(f"è·å–åˆ°éŸ³é¢‘æ•°æ®å—ï¼Œé•¿åº¦: {len(audio_chunk)} å­—èŠ‚")
    oss_output_path = "1"
    # oss_output_path = oss.upload_file_to_oos(audio_file_path+".wav", out_pout_path)
    return oss_output_path, out_pout_path

async def process_audio_data(websocket, toy, connections, web_socket_id):
    # å½“statusä¸º2æ—¶ï¼Œå°†æ‰€æœ‰éŸ³é¢‘æ•°æ®æ±‡æ€»å¹¶ä¿å­˜ä¸ºWAVæ–‡ä»¶
    if web_socket_id not in connections:
        result_json = {"status": 2, "id": toy.client_id}
        try:
            await websocket.send_text(json.dumps(result_json))
        except Exception as e:
            print(1)
            print(f"å‘é€æ•°æ®åˆ°å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
            # await websocket.close(code=1000, reason="Normal closure")
            return
        # connections[toy.client_id].append(decoded_audio)
    combined_audio = b"".join(connections[web_socket_id])
    connections[web_socket_id] = []
    temp_id = int(time.time())
    # ç”ŸæˆWAVæ–‡ä»¶
    output_filename = f"output_{str(temp_id)}.wav"
    with wave.open(output_filename, 'wb') as wf:
        # å‡è®¾éŸ³é¢‘å‚æ•°ä¸ºå•å£°é“ï¼Œé‡‡æ ·ç‡ä¸º16000Hzï¼Œé‡‡æ ·å®½åº¦ä¸º2å­—èŠ‚
        wf.setnchannels(1)
        wf.setsampwidth(2)
        wf.setframerate(16000)
        wf.writeframes(combined_audio)

    print(f"éŸ³é¢‘æ–‡ä»¶å·²ä¿å­˜ä¸º: {output_filename}")
    # è°ƒç”¨æ¨¡å‹è¿›è¡Œæ¨ç†
    result_text = model_inference(output_filename, 'zh', fs=16000)
    text = result_text
    # åˆ é™¤ä¸´æ—¶éŸ³é¢‘æ–‡ä»¶
    if os.path.exists(output_filename):
        os.remove(output_filename)
    toy.is_playing = True
    doubao.start_stream()
    msg = ""
    msg_kongyu = ""
    toy.user_sheding.append({"role": "user", "content": result_text})
    print(toy.user_sheding)
    for chunk in doubao.doubao_chat_stream(toy.user_sheding):
        finish_reason = chunk.choices[0].finish_reason
        message = chunk.choices[0].delta.content
        if finish_reason == "stop":
            break
        msg += message
        msg_kongyu += message
        text_tts, msg_kongyu = doubao.split_text_by_punctuation(msg_kongyu)
        if text_tts != "" and toy.is_playing == True:
            audio_name = uuid.uuid4()
            params = {"text": text_tts, "voice_type": toy.voice_type, "platform": toy.platform}  # æ„é€ å‚æ•°ï¼Œæ ¹æ®å®é™… TTS æ¥å£è°ƒæ•´
            print(params)
            audio_file_path = f"output_audio/{toy.client_id}/{audio_name}"
            # è°ƒç”¨ send_audio_over_ws å‡½æ•°å‘é€éŸ³é¢‘æ•°æ®
            oss_output_path, out_pout_path = await send_audio_over_ws_tengxun(params, audio_file_path)
            if os.path.exists(audio_file_path + ".wav"):
                with wave.open(audio_file_path+".wav", 'rb') as wf:
                    print(f"éŸ³é¢‘æ–‡ä»¶ {out_pout_path} å·²æ‰“å¼€.")

                    # è·å–éŸ³é¢‘æ–‡ä»¶å‚æ•°
                    chunk_size = 1024  # æ¯æ¬¡å‘é€çš„æ•°æ®å—å¤§å°
                    n_channels = wf.getnchannels()
                    sampwidth = wf.getsampwidth()
                    framerate = wf.getframerate()

                    print(f"éŸ³é¢‘å‚æ•°: {n_channels} é€šé“, {sampwidth} å­—èŠ‚æ ·æœ¬å®½åº¦, {framerate} é‡‡æ ·ç‡.")
                    result_json = {"status": 1, "id": toy.client_id}
                    try:
                        await websocket.send_text(json.dumps(result_json))
                    except Exception as e:
                        print(2)
                        # await websocket.close(code=1000, reason="Normal closure")
                        print(f"å‘é€æ•°æ®åˆ°å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
                        return
                    # é€å—è¯»å–éŸ³é¢‘æ•°æ®å¹¶å‘é€ç»™å®¢æˆ·ç«¯
                    while (toy.is_playing == True):
                        audio_data = wf.readframes(chunk_size)
                        if not audio_data:  # å¦‚æœè¯»å–å®Œæ‰€æœ‰æ•°æ®ï¼Œé€€å‡ºå¾ªç¯
                            break
                        # å‘é€éŸ³é¢‘æ•°æ®ç»™å®¢æˆ·ç«¯
                        try:
                            await websocket.send_bytes(audio_data)
                        except Exception as e:
                            print(3)
                            print(f"å‘é€æ•°æ®åˆ°å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
                            # await websocket.close(code=1000, reason="Normal closure")
                            # async with lock:
                            toy.is_playing = False
                            doubao.stop_stream()
                            break
                        await asyncio.sleep(0.001)  # ç¡®ä¿éé˜»å¡å‘é€ï¼Œæ§åˆ¶å‘é€é€Ÿåº¦\
            if toy.is_playing == False:
                print("toy.is_playing"+str(toy.is_playing))
                doubao.stop_stream()
                return
            if os.path.exists(output_filename):
                os.remove(output_filename)
    toy.user_sheding.append({"role": "system", "content": msg})
    # await websocket.send_text(json.dumps({"status": 1, "data": "", "id": client_id}))
    result_json = {"status": 2, "id": toy.client_id}
    try:
        await websocket.send_text(json.dumps(result_json))
    except Exception as e:
        print(4)
        print(f"å‘é€æ•°æ®åˆ°å®¢æˆ·ç«¯æ—¶å‡ºé”™: {e}")
        return

import configparser

def write_to_ini(file_path, section, key_value_dict):
    """
    æ‰“å¼€æŒ‡å®šçš„ ini æ–‡ä»¶å¹¶å†™å…¥å¤šä¸ªé”®å€¼å¯¹ã€‚

    :param file_path: ini æ–‡ä»¶çš„è·¯å¾„
    :param section: è¦å†™å…¥æ•°æ®çš„èŠ‚åç§°
    :param key_value_dict: åŒ…å«å¤šä¸ªé”®å€¼å¯¹çš„å­—å…¸
    """
    config = configparser.ConfigParser()

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è¯»å–å†…å®¹
    if os.path.exists(file_path):
        config.read(file_path)

    # æ£€æŸ¥èŠ‚æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ·»åŠ 
    if not config.has_section(section):
        config.add_section(section)

    # éå†å­—å…¸å¹¶è®¾ç½®é”®å€¼å¯¹
    for key, value in key_value_dict.items():
        config.set(section, key, str(value))

    # å°†ä¿®æ”¹åçš„å†…å®¹å†™å›æ–‡ä»¶
    with open(file_path, 'w') as configfile:
        config.write(configfile)

def read_from_ini(file_path, section=None):
    """
    è¯»å–æŒ‡å®šçš„ ini æ–‡ä»¶ï¼Œæ ¹æ®æä¾›çš„èŠ‚åç§°è¿”å›å¯¹åº”èŠ‚ä¸‹çš„é”®å€¼å¯¹ã€‚
    å¦‚æœæœªæä¾›èŠ‚åç§°ï¼Œåˆ™è¿”å›æ‰€æœ‰èŠ‚åç§°ä»¥åŠå¯¹åº”èŠ‚ä¸‹çš„é”®å€¼å¯¹ã€‚

    :param file_path: ini æ–‡ä»¶çš„è·¯å¾„
    :param section: è¦æŸ¥è¯¢çš„èŠ‚åç§°ï¼Œå¯é€‰å‚æ•°
    :return: è‹¥æä¾›èŠ‚åç§°ï¼Œè¿”å›è¯¥èŠ‚ä¸‹çš„é”®å€¼å¯¹å­—å…¸ï¼›è‹¥æœªæä¾›ï¼Œè¿”å›æ‰€æœ‰èŠ‚åŠå¯¹åº”é”®å€¼å¯¹çš„å­—å…¸
    """
    config = configparser.ConfigParser()
    result = {}

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœå­˜åœ¨åˆ™è¯»å–å†…å®¹
    if os.path.exists(file_path):
        config.read(file_path)

        if section is not None:
            # å¦‚æœæä¾›äº†èŠ‚åç§°ï¼Œæ£€æŸ¥èŠ‚æ˜¯å¦å­˜åœ¨
            if config.has_section(section):
                for key, value in config.items(section):
                    result[key] = value
        else:
            # æœªæä¾›èŠ‚åç§°ï¼Œéå†æ‰€æœ‰èŠ‚
            for sec in config.sections():
                section_dict = {}
                for key, value in config.items(sec):
                    section_dict[key] = value
                result[sec] = section_dict

    return result

app = FastAPI()

# ç”¨äºå­˜å‚¨æ¯ä¸ªè¿æ¥çš„éŸ³é¢‘æ•°æ®
connections = {}


@app.websocket("/ws/transcribe_chat/{client_id}/{web_socket_id}")
async def websocket_endpoint(websocket: WebSocket, client_id: str, web_socket_id: str):
    # web_socket_id = id(websocket)
    try:
        os.makedirs(f"output_audio/{client_id}") if not os.path.exists(f"output_audio/{client_id}") else None
        connections[web_socket_id] = []
        doubao.start_stream()
        toy_dict = read_from_ini("config.ini", client_id)
        if toy_dict == {}:
            toy_dict = {"user_sheding": [],
                        "prompt_text": """ä½ æ˜¯ï¼šè‹ç‘¶
                                        å¹´é¾„ï¼š17 å²èº«ä»½ï¼šå°±è¯»äºå¸‚é‡ç‚¹é«˜ä¸­çš„é«˜äºŒå­¦ç”Ÿï¼Œæ‹…ä»»å­¦æ ¡èˆè¹ˆç¤¾å›¢å›¢é•¿ï¼Œæˆç»©åœ¨å¹´çº§ä¸­ä¸Šæ¸¸ã€‚äºŒã€å¤–è²Œç‰¹å¾ï¼š""",
                        "sound_id": 601012,
                        "platform": "tx",
                        "speech_summary": ""}

        toy = TOY()
        toy.client_id = client_id
        if toy_dict["user_sheding"] != "":
            toy.user_sheding.append({"role": "system", "content": toy_dict["prompt_text"]})
        else:
            json_str = toy_dict["user_sheding"].replace("'", "\"")
            list_data = json.loads(json_str)
            toy.user_sheding = list_data
        toy.prompt_text = toy_dict["prompt_text"]
        toy.is_playing = False
        toy.voice_type = toy_dict["sound_id"]
        # å¹³å°åˆ†ä¸ºæ˜Ÿç«å’Œè…¾è®¯äº‘æ˜Ÿç«å¤§æ¨¡å‹ä¸ºå¤åˆ»å£°éŸ³ï¼Œè…¾è®¯äº‘ä¸ºåŸç”Ÿå£°éŸ³
        toy.platform = toy_dict["platform"]
    except Exception as e:
        print(e)
        return
    try:
        await websocket.accept()
        import asyncio
        while True:
            data = await websocket.receive_text()
            # print(data)
            json_data = json.loads(data)  # å°è¯•è§£æ JSON æ•°æ®
            # å¤„ç†è§£æåçš„æ•°æ®
            print(json_data)

            status = json_data["data"]["status"]
            if status == 0:
                toy.is_playing = False
            if "audio" in json_data["data"]:
                audio_data = json_data["data"]["audio"]
            else:
                audio_data = ""
            if audio_data != "":
                # å°†Base64ç¼–ç çš„éŸ³é¢‘æ•°æ®è§£ç 
                decoded_audio = base64.b64decode(audio_data)
                if status == 1:
                    # å½“statusä¸º1æ—¶ï¼Œç»§ç»­æ”¶é›†éŸ³é¢‘æ•°æ®
                    if web_socket_id not in connections:
                        connections[web_socket_id] = []
                    connections[web_socket_id].append(decoded_audio)
                elif status == 2:
                    asyncio.create_task(process_audio_data(websocket, toy, connections, web_socket_id))
    except WebSocketDisconnect:
        # if 'send_task' in locals():
        #     send_task.cancel()
        # å®¢æˆ·ç«¯æ–­å¼€è¿æ¥æ—¶ï¼Œæ¸…é™¤è¯¥è¿æ¥çš„éŸ³é¢‘æ•°æ®
        if web_socket_id in connections:
            del connections[web_socket_id]
        print(f"å®¢æˆ·ç«¯ {web_socket_id} å·²æ–­å¼€è¿æ¥")
        speech_summary = doubao.doubao_zongjie(toy.user_sheding)
        toy_dict["speech_summary"] = speech_summary
        toy_dict["user_sheding"] = toy.user_sheding
        write_to_ini("config.ini", client_id, toy_dict)

if __name__ == "__main__":
    import uvicorn

    uvicorn.run(app, host="0.0.0.0", port=27000, timeout_keep_alive=10)